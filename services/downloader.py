import asyncio
import aiohttp
import aiofiles
import os
import hashlib
import json
import zipfile
from urllib.parse import urlparse, urljoin
from bs4 import BeautifulSoup
from playwright.async_api import async_playwright
from pathlib import Path
import magic
from datetime import datetime, timedelta
import psutil
import gc
import weakref
from typing import Dict, Optional, Callable, Any

# Ø§Ø³ØªÙŠØ±Ø§Ø¯Ø§Øª Ù…Ø·Ù„Ù‚Ø© Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† Ù†Ø³Ø¨ÙŠØ©
from utils.logger import logger
from utils.helpers import sanitize_filename, human_readable_size
from services.cache_manager import cache_manager
from services.security_manager import security_manager
import config

class WebsiteDownloader:
    def __init__(self):
        self.playwright = None
        self.browser = None
        self.context = None
        self.session = None
        self.downloaded_files = set()
        self.total_size = 0
        self.total_files = 0
        self.progress_callback = None
        self.cancel_event = asyncio.Event()
        self.memory_limit = config.Config.MAX_MEMORY_USAGE
        self._contexts_pool = []
        self._max_contexts = 3
        self._current_context_index = 0
        
    async def initialize(self):
        """ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù…ØªØµÙØ­ ÙˆØ¬Ù„Ø³Ø© HTTP Ù…Ø¹ Ø¥Ø¯Ø§Ø±Ø© Ù…Ø­Ø³Ù†Ø© Ù„Ù„Ø°Ø§ÙƒØ±Ø©"""
        try:
            self.playwright = await async_playwright().start()
            
            # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù…ØªØµÙØ­ Ø§Ù„Ù…Ø­Ø³Ù†Ø© Ù„ØªÙ‚Ù„ÙŠÙ„ Ø§Ø³ØªÙ‡Ù„Ø§Ùƒ Ø§Ù„Ø°Ø§ÙƒØ±Ø©
            self.browser = await self.playwright.chromium.launch(
                headless=True,
                args=[
                    '--disable-gpu',
                    '--disable-dev-shm-usage',
                    '--no-sandbox',
                    '--disable-setuid-sandbox',
                    '--disable-accelerated-2d-canvas',
                    '--no-first-run',
                    '--no-zygote',
                    '--single-process',
                    '--disable-extensions',
                    '--disable-software-rasterizer',
                    '--disable-notifications',
                    '--mute-audio',
                    '--disable-background-timer-throttling',
                    '--disable-backgrounding-occluded-windows',
                    '--disable-breakpad',
                    '--disable-client-side-phishing-detection',
                    '--disable-component-extensions-with-background-pages',
                    '--disable-default-apps',
                    '--disable-hang-monitor',
                    '--disable-ipc-flooding-protection',
                    '--disable-popup-blocking',
                    '--disable-prompt-on-repost',
                    '--disable-renderer-backgrounding',
                    '--disable-sync',
                    '--metrics-recording-only',
                    '--no-default-browser-check',
                    '--use-fake-ui-for-media-stream',
                    '--window-size=1280,720',
                    '--memory-pressure-off',
                    '--max_old_space_size=512',
                    '--disable-background-networking',
                    '--disable-default-apps',
                    '--disable-translate'
                ]
            )
            
            # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…Ù† Ø§Ù„Ø³ÙŠØ§Ù‚Ø§Øª Ù„Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
            await self._create_contexts_pool()
            
            # Ø¥Ù†Ø´Ø§Ø¡ Ø¬Ù„Ø³Ø© HTTP Ù…Ø¹ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ù…Ø­Ø³Ù†Ø©
            connector = aiohttp.TCPConnector(
                limit=100,
                limit_per_host=10,
                ttl_dns_cache=300,
                use_dns_cache=True,
                keepalive_timeout=30,
                enable_cleanup_closed=True
            )
            
            timeout = aiohttp.ClientTimeout(total=300, connect=30)
            self.session = aiohttp.ClientSession(
                timeout=timeout,
                connector=connector,
                headers={
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                }
            )
            
            logger.info("âœ… ØªÙ… ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù…ØªØµÙØ­ ÙˆØ¬Ù„Ø³Ø© HTTP Ø¨Ù†Ø¬Ø§Ø­")
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù…ØªØµÙØ­: {e}")
            await self.close()
            raise
        
    async def close(self):
        """Ø¥ØºÙ„Ø§Ù‚ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯ Ù…Ø¹ ØªÙ†Ø¸ÙŠÙ Ø´Ø§Ù…Ù„ Ù„Ù„Ø°Ø§ÙƒØ±Ø©"""
        try:
            # Ø¥ØºÙ„Ø§Ù‚ Ø¬Ù„Ø³Ø© HTTP
            if self.session and not self.session.closed:
                await self.session.close()
                await asyncio.sleep(0.1)  # Ø§Ù†ØªØ¸Ø§Ø± Ù‚ØµÙŠØ± Ù„Ù„ØªÙ†Ø¸ÙŠÙ
            
            # Ø¥ØºÙ„Ø§Ù‚ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø³ÙŠØ§Ù‚Ø§Øª
            for context in self._contexts_pool:
                try:
                    await context.close()
                except Exception as e:
                    logger.warning(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ Ø¥ØºÙ„Ø§Ù‚ Ø§Ù„Ø³ÙŠØ§Ù‚: {e}")
            
            self._contexts_pool.clear()
            
            # Ø¥ØºÙ„Ø§Ù‚ Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
            if self.context:
                try:
                    await self.context.close()
                except Exception as e:
                    logger.warning(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ Ø¥ØºÙ„Ø§Ù‚ Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ: {e}")
            
            # Ø¥ØºÙ„Ø§Ù‚ Ø§Ù„Ù…ØªØµÙØ­
            if self.browser:
                try:
                    await self.browser.close()
                except Exception as e:
                    logger.warning(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ Ø¥ØºÙ„Ø§Ù‚ Ø§Ù„Ù…ØªØµÙØ­: {e}")
            
            # Ø¥ÙŠÙ‚Ø§Ù Playwright
            if self.playwright:
                try:
                    await self.playwright.stop()
                except Exception as e:
                    logger.warning(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ Ø¥ÙŠÙ‚Ø§Ù Playwright: {e}")
            
            # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª
            self.session = None
            self.context = None
            self.browser = None
            self.playwright = None
            
            # ØªØ´ØºÙŠÙ„ Ø¬Ø§Ù…Ø¹ Ø§Ù„Ù‚Ù…Ø§Ù…Ø©
            gc.collect()
            
            logger.info("âœ… ØªÙ… Ø¥ØºÙ„Ø§Ù‚ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯ Ø¨Ù†Ø¬Ø§Ø­")
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø¥ØºÙ„Ø§Ù‚ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯: {e}")
    
    async def _create_contexts_pool(self):
        """Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…Ù† Ø§Ù„Ø³ÙŠØ§Ù‚Ø§Øª Ù„Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…"""
        for i in range(self._max_contexts):
            context = await self.browser.new_context(
                viewport={'width': 1280, 'height': 720},
                device_scale_factor=1,
                is_mobile=False,
                has_touch=False,
                java_script_enabled=True,
                locale='en-US',
                timezone_id='UTC',
                ignore_https_errors=True,
                bypass_csp=True
            )
            
            # ØªØ¹Ø·ÙŠÙ„ Ø·Ù„Ø¨Ø§Øª Ø§Ù„Ù…ÙˆØ§Ø±Ø¯ ØºÙŠØ± Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ©
            await context.route('**/*.{png,jpg,jpeg,gif,svg,woff,woff2,ttf,eot,ico}', 
                              lambda route: route.abort())
            
            self._contexts_pool.append(context)
    
    async def _get_context(self):
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø³ÙŠØ§Ù‚ Ù…Ù† Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø©"""
        context = self._contexts_pool[self._current_context_index]
        self._current_context_index = (self._current_context_index + 1) % self._max_contexts
        return context
    
    async def _check_memory_usage(self):
        """ÙØ­Øµ Ø§Ø³ØªÙ‡Ù„Ø§Ùƒ Ø§Ù„Ø°Ø§ÙƒØ±Ø©"""
        process = psutil.Process()
        memory_info = process.memory_info()
        memory_mb = memory_info.rss / 1024 / 1024
        
        if memory_mb > self.memory_limit:
            logger.warning(f"âš ï¸ Ø§Ø³ØªÙ‡Ù„Ø§Ùƒ Ø°Ø§ÙƒØ±Ø© Ø¹Ø§Ù„ÙŠ: {memory_mb:.1f}MB")
            # ØªØ´ØºÙŠÙ„ Ø¬Ø§Ù…Ø¹ Ø§Ù„Ù‚Ù…Ø§Ù…Ø©
            gc.collect()
            return False
        return True
    
    def set_progress_callback(self, callback: Callable[[float, str], None]):
        """ØªØ¹ÙŠÙŠÙ† Ø¯Ø§Ù„Ø© ØªØ­Ø¯ÙŠØ« Ø§Ù„ØªÙ‚Ø¯Ù…"""
        self.progress_callback = callback
    
    def cancel_download(self):
        """Ø¥Ù„ØºØ§Ø¡ Ø§Ù„ØªÙ†Ø²ÙŠÙ„"""
        self.cancel_event.set()
    
    async def _update_progress(self, progress: float, message: str = ""):
        """ØªØ­Ø¯ÙŠØ« Ø§Ù„ØªÙ‚Ø¯Ù…"""
        if self.progress_callback:
            try:
                await self.progress_callback(progress, message)
            except Exception as e:
                logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ø¯ÙŠØ« Ø§Ù„ØªÙ‚Ø¯Ù…: {e}")
    
    async def download_website(self, url, output_dir, max_depth=2, max_size=50*1024*1024, user_id=None):
        """ØªÙ†Ø²ÙŠÙ„ Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ø¨Ø§Ù„ÙƒØ§Ù…Ù„ Ù…Ø¹ Ø¯Ø¹Ù… Ø§Ù„ÙƒØ§Ø´ ÙˆØ§Ù„Ø£Ù…Ø§Ù†"""
        try:
            # ÙØ­Øµ Ø§Ù„Ø£Ù…Ø§Ù†
            if user_id:
                security_check = await security_manager.validate_url_security(url, user_id)
                if not security_check['is_safe']:
                    raise Exception(f"Ø±Ø§Ø¨Ø· ØºÙŠØ± Ø¢Ù…Ù†: {', '.join(security_check['threats'])}")
            
            # ÙØ­Øµ Ø§Ù„ÙƒØ§Ø´
            cache_key = f"website_{hashlib.md5(url.encode()).hexdigest()}"
            cached_result = await cache_manager.get(cache_key)
            
            if cached_result:
                logger.info(f"ğŸ“¦ ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù†Ø³Ø®Ø© Ù…Ø®Ø²Ù†Ø© Ù„Ù„Ù…ÙˆÙ‚Ø¹: {url}")
                await self._update_progress(100.0, "ØªÙ… Ø§Ø³ØªØ±Ø¯Ø§Ø¯ Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ù…Ù† Ø§Ù„ÙƒØ§Ø´")
                return cached_result['path'], cached_result['files'], cached_result['size']
            
            await self._update_progress(5.0, "Ø¨Ø¯Ø¡ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ÙˆÙ‚Ø¹...")
            
            parsed_url = urlparse(url)
            base_domain = parsed_url.netloc
            base_url = f"{parsed_url.scheme}://{parsed_url.netloc}"
            
            # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù„Ø¯ Ø§Ù„ØªÙ†Ø²ÙŠÙ„
            domain_dir = os.path.join(output_dir, sanitize_filename(base_domain))
            os.makedirs(domain_dir, exist_ok=True)
            
            await self._update_progress(10.0, "ØªÙ†Ø²ÙŠÙ„ Ø§Ù„ØµÙØ­Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©...")
            
            # ØªÙ†Ø²ÙŠÙ„ Ø§Ù„ØµÙØ­Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ø£ÙˆÙ„Ø§Ù‹
            main_page_path = await self.download_page(url, domain_dir, base_url)
            
            if main_page_path:
                await self._update_progress(30.0, "Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø±ÙˆØ§Ø¨Ø·...")
                
                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ù…Ù† Ø§Ù„ØµÙØ­Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
                links = await self.extract_links(url, domain_dir, base_url)
                
                # ØªÙ†Ø²ÙŠÙ„ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠØ©
                total_links = min(len(links), 10)  # Ø­Ø¯ 10 ØµÙØ­Ø§Øª Ù„Ù„Ø¨Ø¯Ø§ÙŠØ©
                
                for i, link in enumerate(links[:total_links]):
                    if self.cancel_event.is_set():
                        logger.info("ğŸš« ØªÙ… Ø¥Ù„ØºØ§Ø¡ Ø§Ù„ØªÙ†Ø²ÙŠÙ„")
                        break
                    
                    if self.total_size < max_size:
                        progress = 30 + (i / total_links) * 60
                        await self._update_progress(progress, f"ØªÙ†Ø²ÙŠÙ„ Ø§Ù„ØµÙØ­Ø© {i+1}/{total_links}...")
                        await self.download_page(link, domain_dir, base_url)
                        
                        # ÙØ­Øµ Ø§Ø³ØªÙ‡Ù„Ø§Ùƒ Ø§Ù„Ø°Ø§ÙƒØ±Ø©
                        if not await self._check_memory_usage():
                            logger.warning("âš ï¸ ØªÙ… Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„ØªÙ†Ø²ÙŠÙ„ Ø¨Ø³Ø¨Ø¨ Ø§Ø³ØªÙ‡Ù„Ø§Ùƒ Ø§Ù„Ø°Ø§ÙƒØ±Ø©")
                            break
            
            await self._update_progress(90.0, "Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø£Ø±Ø´ÙŠÙ...")
            
            # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ZIP
            zip_path = await self._create_zip_archive(domain_dir)
            
            # Ø­ÙØ¸ ÙÙŠ Ø§Ù„ÙƒØ§Ø´
            cache_data = {
                'path': zip_path,
                'files': self.total_files,
                'size': self.total_size,
                'created_at': datetime.utcnow().isoformat()
            }
            await cache_manager.set(cache_key, cache_data, ttl=3600)  # ÙƒØ§Ø´ Ù„Ø³Ø§Ø¹Ø© ÙˆØ§Ø­Ø¯Ø©
            
            await self._update_progress(100.0, "ØªÙ… Ø¥ÙƒÙ…Ø§Ù„ Ø§Ù„ØªÙ†Ø²ÙŠÙ„ Ø¨Ù†Ø¬Ø§Ø­")
            
            return zip_path, self.total_files, self.total_size
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªÙ†Ø²ÙŠÙ„ Ø§Ù„Ù…ÙˆÙ‚Ø¹: {e}")
            await self._update_progress(0.0, f"Ø®Ø·Ø£: {str(e)}")
            raise
    
    async def download_page(self, url, output_dir, base_url):
        """ØªÙ†Ø²ÙŠÙ„ ØµÙØ­Ø© ÙØ±Ø¯ÙŠØ© Ù…Ø¹ Ø¥Ø¯Ø§Ø±Ø© Ù…Ø­Ø³Ù†Ø© Ù„Ù„Ø°Ø§ÙƒØ±Ø©"""
        try:
            if url in self.downloaded_files or self.cancel_event.is_set():
                return None
                
            self.downloaded_files.add(url)
            
            # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø³ÙŠØ§Ù‚ Ù…Ù† Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø©
            context = await self._get_context()
            page = await context.new_page()
            
            try:
                # ØªØ¹ÙŠÙŠÙ† Ù…Ù‡Ù„Ø© Ø£Ø·ÙˆÙ„ Ù„Ù„ØµÙØ­Ø§Øª Ø§Ù„Ø«Ù‚ÙŠÙ„Ø©
                await page.goto(url, timeout=60000, wait_until='domcontentloaded')
                
                # Ø§Ù†ØªØ¸Ø§Ø± ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙØ­Ø© Ù…Ø¹ Ù…Ù‡Ù„Ø© Ù‚ØµÙŠØ±Ø©
                try:
                    await page.wait_for_load_state('networkidle', timeout=15000)
                except Exception:
                    # Ø§Ù„Ù…ØªØ§Ø¨Ø¹Ø© Ø­ØªÙ‰ Ù„Ùˆ Ù„Ù… ØªÙƒØªÙ…Ù„ Ø§Ù„Ø´Ø¨ÙƒØ©
                    pass
                
                # ØªØ­Ø³ÙŠÙ† Ø§Ù„ØµÙØ­Ø© ÙˆØªÙ‚Ù„ÙŠÙ„ Ø­Ø¬Ù…Ù‡Ø§
                await page.evaluate("""() => {
                    // Ø­Ø°Ù Ø§Ù„Ø¹Ù†Ø§ØµØ± ØºÙŠØ± Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ©
                    const selectors = [
                        'script[src*="analytics"]', 'script[src*="gtag"]', 'script[src*="facebook"]',
                        'iframe[src*="youtube"]', 'iframe[src*="twitter"]', 'iframe[src*="instagram"]',
                        '.advertisement', '.ads', '.social-share', '.popup', '.modal',
                        'header', 'footer', 'nav', '[role="banner"]', '[role="navigation"]'
                    ];
                    
                    selectors.forEach(selector => {
                        document.querySelectorAll(selector).forEach(el => el.remove());
                    });
                    
                    // ØªØ­Ø³ÙŠÙ† Ø§Ù„ØµÙˆØ±
                    document.querySelectorAll('img').forEach(img => {
                        img.loading = 'lazy';
                        if (img.width > 600) {
                            img.width = 600;
                            img.height = 'auto';
                        }
                        // Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØµÙˆØ± Ø§Ù„ÙƒØ¨ÙŠØ±Ø© Ø¬Ø¯Ø§Ù‹
                        if (img.naturalWidth > 2000 || img.naturalHeight > 2000) {
                            img.remove();
                        }
                    });
                    
                    // ØªÙ†Ø¸ÙŠÙ CSS ØºÙŠØ± Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
                    document.querySelectorAll('style').forEach(style => {
                        if (style.textContent.length > 50000) {
                            style.remove();
                        }
                    });
                }""")
                
            except Exception as e:
                logger.warning(f"âš ï¸ ØªØ­Ø°ÙŠØ± Ø£Ø«Ù†Ø§Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙØ­Ø©: {e}")
            
            # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ HTML Ø¨Ø¹Ø¯ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
            content = await page.content()
            
            # Ø­ÙØ¸ HTML
            parsed_url = urlparse(url)
            filename = sanitize_filename(parsed_url.path or "index") + ".html"
            if filename == ".html":
                filename = "index.html"
                
            filepath = os.path.join(output_dir, filename)
            
            # Ø¶ØºØ· Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø¥Ø°Ø§ ÙƒØ§Ù† ÙƒØ¨ÙŠØ±Ø§Ù‹
            if len(content) > 1024 * 1024:  # 1MB
                content = await self._compress_html(content)
            
            async with aiofiles.open(filepath, 'w', encoding='utf-8') as f:
                await f.write(content)
            
            # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
            file_size = os.path.getsize(filepath)
            self.total_size += file_size
            self.total_files += 1
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆØªÙ†Ø²ÙŠÙ„ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯ Ø§Ù„Ù…Ù‡Ù…Ø© ÙÙ‚Ø·
            await self.download_resources(content, output_dir, base_url)
            
            await page.close()
            
            # ÙØ­Øµ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø¨Ø¹Ø¯ ÙƒÙ„ ØµÙØ­Ø©
            await self._check_memory_usage()
            
            return filepath
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªÙ†Ø²ÙŠÙ„ Ø§Ù„ØµÙØ­Ø© {url}: {e}")
            return None
    
    async def _compress_html(self, html_content: str) -> str:
        """Ø¶ØºØ· Ù…Ø­ØªÙˆÙ‰ HTML"""
        try:
            # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù…Ø³Ø§ÙØ§Øª Ø§Ù„Ø²Ø§Ø¦Ø¯Ø© ÙˆØ§Ù„ØªØ¹Ù„ÙŠÙ‚Ø§Øª
            import re
            
            # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªØ¹Ù„ÙŠÙ‚Ø§Øª
            html_content = re.sub(r'<!--.*?-->', '', html_content, flags=re.DOTALL)
            
            # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù…Ø³Ø§ÙØ§Øª Ø§Ù„Ø²Ø§Ø¦Ø¯Ø©
            html_content = re.sub(r'\s+', ' ', html_content)
            
            # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù…Ø³Ø§ÙØ§Øª Ø­ÙˆÙ„ Ø§Ù„Ø¹Ù„Ø§Ù…Ø§Øª
            html_content = re.sub(r'>\s+<', '><', html_content)
            
            return html_content.strip()
            
        except Exception as e:
            logger.warning(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ Ø¶ØºØ· HTML: {e}")
            return html_content
    
    async def _create_zip_archive(self, directory_path: str) -> str:
        """Ø¥Ù†Ø´Ø§Ø¡ Ø£Ø±Ø´ÙŠÙ ZIP Ù„Ù„Ù…Ø¬Ù„Ø¯"""
        try:
            zip_path = f"{directory_path}.zip"
            
            with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED, compresslevel=6) as zipf:
                for root, dirs, files in os.walk(directory_path):
                    for file in files:
                        file_path = os.path.join(root, file)
                        arc_name = os.path.relpath(file_path, directory_path)
                        zipf.write(file_path, arc_name)
            
            # Ø­Ø°Ù Ø§Ù„Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø£ØµÙ„ÙŠ Ù„ØªÙˆÙÙŠØ± Ø§Ù„Ù…Ø³Ø§Ø­Ø©
            import shutil
            shutil.rmtree(directory_path)
            
            return zip_path
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø£Ø±Ø´ÙŠÙ: {e}")
            return directory_path
    
    async def download_resources(self, html_content, output_dir, base_url):
        """ØªÙ†Ø²ÙŠÙ„ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯ Ø§Ù„Ù…Ø±ØªØ¨Ø·Ø© Ø¨Ø§Ù„ØµÙØ­Ø©"""
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # Ø±ÙˆØ§Ø¨Ø· CSS
        for link in soup.find_all('link', rel='stylesheet'):
            href = link.get('href')
            if href:
                await self.download_resource(href, output_dir, base_url, 'css')
        
        # Ø³ÙƒØ±ÙŠØ¨ØªØ§Øª JS
        for script in soup.find_all('script', src=True):
            src = script.get('src')
            if src:
                await self.download_resource(src, output_dir, base_url, 'js')
        
        # ØµÙˆØ±
        for img in soup.find_all('img', src=True):
            src = img.get('src')
            if src:
                await self.download_resource(src, output_dir, base_url, 'images')
    
    async def download_resource(self, resource_url, output_dir, base_url, resource_type):
        """ØªÙ†Ø²ÙŠÙ„ Ù…ÙˆØ±Ø¯ ÙØ±Ø¯ÙŠ"""
        try:
            if not resource_url.startswith(('http', '//')):
                resource_url = urljoin(base_url, resource_url)
            
            if resource_url in self.downloaded_files:
                return
                
            self.downloaded_files.add(resource_url)
            
            async with self.session.get(resource_url) as response:
                if response.status == 200:
                    content = await response.read()
                    
                    # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù„Ø¯ Ù„Ù„Ù…ÙˆØ±Ø¯
                    resource_dir = os.path.join(output_dir, resource_type)
                    os.makedirs(resource_dir, exist_ok=True)
                    
                    # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ø³Ù… Ù…Ù„Ù ÙØ±ÙŠØ¯
                    filename = os.path.basename(urlparse(resource_url).path)
                    if not filename:
                        filename = f"resource_{hash(resource_url)}"
                    
                    filepath = os.path.join(resource_dir, filename)
                    
                    async with aiofiles.open(filepath, 'wb') as f:
                        await f.write(content)
                    
                    # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
                    file_size = len(content)
                    self.total_size += file_size
                    self.total_files += 1
                    
        except Exception as e:
            logger.error(f"Error downloading resource {resource_url}: {e}")
    
    async def extract_links(self, url, output_dir, base_url):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ù…Ù† Ø§Ù„ØµÙØ­Ø©"""
        try:
            page = await self.context.new_page()
            await page.goto(url, wait_until='networkidle')
            
            # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠØ©
            links = await page.evaluate('''() => {
                return Array.from(document.querySelectorAll('a[href]'))
                    .map(a => a.href)
                    .filter(href => href.startsWith(window.location.origin))
                    .filter(href => !href.includes('#'))
            }''')
            
            await page.close()
            return list(set(links))  # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª
            
        except Exception as e:
            logger.error(f"Error extracting links: {e}")
            return []
