"""
Ù†Ø¸Ø§Ù… Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª Ø§Ù„Ù…ØªÙ‚Ø¯Ù…
Advanced Cache Management System
"""

import asyncio
import hashlib
import json
import os
import time
from datetime import datetime, timedelta
from pathlib import Path
from typing import Optional, Dict, Any
import aiofiles

from utils.logger import logger
import config

class CacheManager:
    """Ù…Ø¯ÙŠØ± Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª Ø§Ù„Ù…ØªÙ‚Ø¯Ù…"""
    
    def __init__(self):
        self.cache_dir = Path(config.Config.DATA_DIR) / "cache"
        self.cache_dir.mkdir(exist_ok=True)
        
        # ØªØ®Ø²ÙŠÙ† Ù…Ø¤Ù‚Øª ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ù„Ù„ÙˆØµÙˆÙ„ Ø§Ù„Ø³Ø±ÙŠØ¹
        self.memory_cache = {}
        self.cache_stats = {
            'hits': 0,
            'misses': 0,
            'size': 0
        }
        
        # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª
        self.max_memory_cache_size = 100  # Ø¹Ø¯Ø¯ Ø§Ù„Ø¹Ù†Ø§ØµØ±
        self.default_ttl = 3600  # Ø³Ø§Ø¹Ø© ÙˆØ§Ø­Ø¯Ø©
        self.max_file_size = 10 * 1024 * 1024  # 10MB
    
    def _generate_cache_key(self, url: str, options: Dict = None) -> str:
        """Ø¥Ù†Ø´Ø§Ø¡ Ù…ÙØªØ§Ø­ ØªØ®Ø²ÙŠÙ† Ù…Ø¤Ù‚Øª ÙØ±ÙŠØ¯"""
        cache_data = {
            'url': url,
            'options': options or {}
        }
        cache_string = json.dumps(cache_data, sort_keys=True)
        return hashlib.sha256(cache_string.encode()).hexdigest()[:16]
    
    def _get_cache_file_path(self, cache_key: str) -> Path:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª"""
        return self.cache_dir / f"{cache_key}.json"
    
    def _get_data_file_path(self, cache_key: str) -> Path:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        return self.cache_dir / f"{cache_key}.zip"
    
    async def get(self, url: str, options: Dict = None) -> Optional[Dict]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¹Ù†ØµØ± Ù…Ù† Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª"""
        cache_key = self._generate_cache_key(url, options)
        
        # ÙØ­Øµ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø£ÙˆÙ„Ø§Ù‹
        if cache_key in self.memory_cache:
            cache_item = self.memory_cache[cache_key]
            if self._is_cache_valid(cache_item):
                self.cache_stats['hits'] += 1
                logger.debug(f"ğŸ¯ Cache hit (memory): {cache_key}")
                return cache_item['data']
            else:
                # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø¹Ù†ØµØ± Ø§Ù„Ù…Ù†ØªÙ‡ÙŠ Ø§Ù„ØµÙ„Ø§Ø­ÙŠØ©
                del self.memory_cache[cache_key]
        
        # ÙØ­Øµ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª Ø¹Ù„Ù‰ Ø§Ù„Ù‚Ø±Øµ
        cache_file = self._get_cache_file_path(cache_key)
        if cache_file.exists():
            try:
                async with aiofiles.open(cache_file, 'r', encoding='utf-8') as f:
                    cache_data = json.loads(await f.read())
                
                if self._is_cache_valid(cache_data):
                    # Ø¥Ø¶Ø§ÙØ© Ù„Ù„Ø°Ø§ÙƒØ±Ø© Ù„Ù„ÙˆØµÙˆÙ„ Ø§Ù„Ø³Ø±ÙŠØ¹
                    self._add_to_memory_cache(cache_key, cache_data)
                    self.cache_stats['hits'] += 1
                    logger.debug(f"ğŸ¯ Cache hit (disk): {cache_key}")
                    return cache_data['data']
                else:
                    # Ø­Ø°Ù Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ù†ØªÙ‡ÙŠ Ø§Ù„ØµÙ„Ø§Ø­ÙŠØ©
                    await self._remove_cache_files(cache_key)
            
            except Exception as e:
                logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª: {e}")
        
        self.cache_stats['misses'] += 1
        logger.debug(f"âŒ Cache miss: {cache_key}")
        return None
    
    async def set(self, url: str, data: Dict, options: Dict = None, ttl: int = None) -> bool:
        """Ø­ÙØ¸ Ø¹Ù†ØµØ± ÙÙŠ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª"""
        cache_key = self._generate_cache_key(url, options)
        ttl = ttl or self.default_ttl
        
        cache_item = {
            'data': data,
            'created_at': time.time(),
            'ttl': ttl,
            'url': url,
            'options': options or {}
        }
        
        try:
            # Ø­ÙØ¸ ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø©
            self._add_to_memory_cache(cache_key, cache_item)
            
            # Ø­ÙØ¸ Ø¹Ù„Ù‰ Ø§Ù„Ù‚Ø±Øµ
            cache_file = self._get_cache_file_path(cache_key)
            async with aiofiles.open(cache_file, 'w', encoding='utf-8') as f:
                await f.write(json.dumps(cache_item, indent=2))
            
            self.cache_stats['size'] += 1
            logger.debug(f"ğŸ’¾ Cache stored: {cache_key}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø­ÙØ¸ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª: {e}")
            return False
    
    async def cache_file(self, cache_key: str, file_path: str) -> bool:
        """ØªØ®Ø²ÙŠÙ† Ù…Ù„Ù ÙÙŠ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª"""
        try:
            if not os.path.exists(file_path):
                return False
            
            file_size = os.path.getsize(file_path)
            if file_size > self.max_file_size:
                logger.warning(f"âš ï¸ Ù…Ù„Ù ÙƒØ¨ÙŠØ± Ø¬Ø¯Ø§Ù‹ Ù„Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª: {human_readable_size(file_size)}")
                return False
            
            cached_file_path = self._get_data_file_path(cache_key)
            
            # Ù†Ø³Ø® Ø§Ù„Ù…Ù„Ù
            async with aiofiles.open(file_path, 'rb') as src:
                async with aiofiles.open(cached_file_path, 'wb') as dst:
                    await dst.write(await src.read())
            
            logger.debug(f"ğŸ“ File cached: {cache_key}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ù„Ù: {e}")
            return False
    
    async def get_cached_file(self, cache_key: str) -> Optional[str]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ù„Ù Ù…Ù† Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª"""
        cached_file_path = self._get_data_file_path(cache_key)
        
        if cached_file_path.exists():
            return str(cached_file_path)
        
        return None
    
    def _is_cache_valid(self, cache_item: Dict) -> bool:
        """ÙØ­Øµ ØµØ­Ø© Ø¹Ù†ØµØ± Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª"""
        if not cache_item:
            return False
        
        created_at = cache_item.get('created_at', 0)
        ttl = cache_item.get('ttl', self.default_ttl)
        
        return (time.time() - created_at) < ttl
    
    def _add_to_memory_cache(self, cache_key: str, cache_item: Dict):
        """Ø¥Ø¶Ø§ÙØ© Ø¹Ù†ØµØ± Ù„Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø©"""
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø¹Ù†Ø§ØµØ± Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø© Ø¥Ø°Ø§ Ø§Ù…ØªÙ„Ø£Øª Ø§Ù„Ø°Ø§ÙƒØ±Ø©
        if len(self.memory_cache) >= self.max_memory_cache_size:
            # Ø¥Ø²Ø§Ù„Ø© Ø£Ù‚Ø¯Ù… Ø¹Ù†ØµØ±
            oldest_key = min(
                self.memory_cache.keys(),
                key=lambda k: self.memory_cache[k].get('created_at', 0)
            )
            del self.memory_cache[oldest_key]
        
        self.memory_cache[cache_key] = cache_item
    
    async def _remove_cache_files(self, cache_key: str):
        """Ø¥Ø²Ø§Ù„Ø© Ù…Ù„ÙØ§Øª Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª"""
        try:
            cache_file = self._get_cache_file_path(cache_key)
            data_file = self._get_data_file_path(cache_key)
            
            if cache_file.exists():
                cache_file.unlink()
            
            if data_file.exists():
                data_file.unlink()
                
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø­Ø°Ù Ù…Ù„ÙØ§Øª Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª: {e}")
    
    async def clear_expired(self):
        """Ù…Ø³Ø­ Ø§Ù„Ø¹Ù†Ø§ØµØ± Ù…Ù†ØªÙ‡ÙŠØ© Ø§Ù„ØµÙ„Ø§Ø­ÙŠØ©"""
        cleared_count = 0
        
        try:
            # Ù…Ø³Ø­ Ù…Ù† Ø§Ù„Ø°Ø§ÙƒØ±Ø©
            expired_keys = []
            for key, item in self.memory_cache.items():
                if not self._is_cache_valid(item):
                    expired_keys.append(key)
            
            for key in expired_keys:
                del self.memory_cache[key]
                cleared_count += 1
            
            # Ù…Ø³Ø­ Ù…Ù† Ø§Ù„Ù‚Ø±Øµ
            for cache_file in self.cache_dir.glob("*.json"):
                try:
                    async with aiofiles.open(cache_file, 'r', encoding='utf-8') as f:
                        cache_data = json.loads(await f.read())
                    
                    if not self._is_cache_valid(cache_data):
                        cache_key = cache_file.stem
                        await self._remove_cache_files(cache_key)
                        cleared_count += 1
                        
                except Exception as e:
                    logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ÙØ­Øµ Ù…Ù„Ù Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª: {e}")
            
            if cleared_count > 0:
                logger.info(f"ğŸ§¹ ØªÙ… Ù…Ø³Ø­ {cleared_count} Ø¹Ù†ØµØ± Ù…Ù†ØªÙ‡ÙŠ Ø§Ù„ØµÙ„Ø§Ø­ÙŠØ© Ù…Ù† Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª")
                
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ù…Ø³Ø­ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª: {e}")
    
    async def clear_all(self):
        """Ù…Ø³Ø­ Ø¬Ù…ÙŠØ¹ Ø¹Ù†Ø§ØµØ± Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª"""
        try:
            # Ù…Ø³Ø­ Ø§Ù„Ø°Ø§ÙƒØ±Ø©
            self.memory_cache.clear()
            
            # Ù…Ø³Ø­ Ø§Ù„Ù…Ù„ÙØ§Øª
            for file_path in self.cache_dir.glob("*"):
                if file_path.is_file():
                    file_path.unlink()
            
            # Ø¥Ø¹Ø§Ø¯Ø© ØªØ¹ÙŠÙŠÙ† Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
            self.cache_stats = {
                'hits': 0,
                'misses': 0,
                'size': 0
            }
            
            logger.info("ğŸ§¹ ØªÙ… Ù…Ø³Ø­ Ø¬Ù…ÙŠØ¹ Ø¹Ù†Ø§ØµØ± Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª")
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ù…Ø³Ø­ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª: {e}")
    
    def get_stats(self) -> Dict:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª"""
        total_requests = self.cache_stats['hits'] + self.cache_stats['misses']
        hit_rate = (self.cache_stats['hits'] / total_requests * 100) if total_requests > 0 else 0
        
        return {
            'hits': self.cache_stats['hits'],
            'misses': self.cache_stats['misses'],
            'hit_rate': f"{hit_rate:.1f}%",
            'memory_cache_size': len(self.memory_cache),
            'disk_cache_size': len(list(self.cache_dir.glob("*.json"))),
            'total_size': self.cache_stats['size']
        }
    
    async def get_cache_size(self) -> Dict:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø­Ø¬Ù… Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª"""
        try:
            total_size = 0
            file_count = 0
            
            for file_path in self.cache_dir.glob("*"):
                if file_path.is_file():
                    total_size += file_path.stat().st_size
                    file_count += 1
            
            return {
                'total_size': total_size,
                'formatted_size': human_readable_size(total_size),
                'file_count': file_count
            }
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø­Ø³Ø§Ø¨ Ø­Ø¬Ù… Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª: {e}")
            return {'total_size': 0, 'formatted_size': '0B', 'file_count': 0}

# Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø«ÙŠÙ„ Ø¹Ø§Ù… Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
cache_manager = CacheManager()
